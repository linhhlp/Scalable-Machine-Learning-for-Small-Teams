{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c0aea3",
   "metadata": {},
   "source": [
    "# Containers & Batch/Streaming Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0adc0e",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Containers\n",
    "\n",
    "### 1.1 Benefits \n",
    "\n",
    "* Lightweight alternative to virtual machines, which provide similar functionality.\n",
    "* Faster to spin up, while providing the same level of isolation as virtual machines.\n",
    "* Can reuse layers from other containers, making it much faster to build and share containers.\n",
    "* Great solution to use when you need to run conflicting versions of Python runtimes or libraries, on a single machine.\n",
    "\n",
    "### 1.2 Data Science\n",
    "\n",
    "* Reproducible analyses: after packaging up analyses, others can rerun your work months or years later.\n",
    "* Web applications: we can build an interactive web application in which Containers provide a great way of abstracting away hosting concerns for deploying the app.\n",
    "* Model deployments: To let the model as an endpoint, containers provide a great way of separating the model application code from the model serving infrastructure.\n",
    "\n",
    "### 1.3 Serverless Function vs. Container Technologies\n",
    "\n",
    "The best approach for serving models depends on your deployment environment and expected workload. Typically, you are constrained to a specific cloud platform when working at a company, because your model service may need to interface with other components in the cloud, such as a database or cloud storage. Within AWS, there are multiple options for hosting containers while GCP is aligned with GKE as a single solution. The main question to ask is whether it is more cost-effective to serve your model using serverless function technologies or elastic container technologies.\n",
    "\n",
    "The correct answer depends on the volume of traffic you need to handle, the amount of latency that is tolerable for end-users, and the complexity of models that you need to host. Containerized solutions are great for serving complex models and making sure that you can meet latency requirements, but they may require a bit more DevOps overhead versus serverless functions.\n",
    "\n",
    "### 1.4 Container Orchestration\n",
    "\n",
    "Container orchestration systems are responsible for managing the life cycles of containers in a cluster. They provide services including provisioning, scaling, failover, load balancing, and service discovery between containers. \n",
    "\n",
    "the general trend has been moving towards Kubernetes for this functionality, which is an open-source platform originally designed by Google.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de556f",
   "metadata": {},
   "source": [
    "### 1.5 Wrap the Application in a Docker Container\n",
    "\n",
    "#### Dockerfile\n",
    "\n",
    "* The first step is to use the `FROM` command to identify a base image to use. The ubuntu image provides a Linux environment that supports the apt-get command. \n",
    "* The `MAINTAINER` command adds to the metadata information associated with the image, adding the name of the image maintainer. * The `RUN` command is used to install Python, set up a symbolic link, and install Flask. For containers with many Python libraries, itâ€™s also possible to use a requirements.txt file. \n",
    "* The `COPY` and `ADD` command inserts our script into the image and places the files and folders in the root directory. \n",
    "* The final command could be `RUN`, `CMD`, or `ENTRYPOINT`, which specifies which arguments to run to execute the application. [Read more here](https://awstip.com/docker-run-vs-cmd-vs-entrypoint-78ca2e5472bd)\n",
    "\n",
    "```dos\n",
    "FROM ubuntu:latest\n",
    "MAINTAINER Linh H\n",
    "\n",
    "RUN apt-get update \\  \n",
    "  && apt-get install -y python3.9 python3-pip \\  \n",
    "  && cd /usr/local/bin \\  \n",
    "  && ln -s /usr/bin/python3 python \\  \n",
    "  && pip3 install flask==2.0 pandas==1.4 mlflow==2.1 scikit-learn==1.0.2 statsmodels==0.13.2\n",
    "  \n",
    "COPY echo_docker.py echo.py \n",
    "ADD models models\n",
    "ADD db db\n",
    "\n",
    "ENTRYPOINT [\"gunicorn\",\"echo:app\", \"-b\", \"0.0.0.0:80\"]\n",
    "```\n",
    "\n",
    "#### Creating an Image\n",
    "\n",
    "Let's build an image, named `echo_service`, for the current folder. You need Docker daemon services running background.\n",
    "\n",
    "```\n",
    "$ docker image build -t \"echo_service\" .\n",
    "```\n",
    "\n",
    "#### Running Containers\n",
    "\n",
    "After successfully building the image, we can start running this image as a container to serve the predictive model at port 8080.\n",
    "\n",
    "```\n",
    "$ sudo docker run -d -p 8080:80 echo_service\n",
    "```\n",
    "\n",
    "* The -d flag specifies that the container should run as a daemon process, which will continue to run even when shutting down the terminal. \n",
    "* The -p flag is used to map a port on the host machine to a port that the container uses for communication. Port mapping from the host port of 8080 to the container port 80 (by gunicorn)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd396944",
   "metadata": {},
   "source": [
    "### 1.6 Run Container in Kubernetes on GCP\n",
    "\n",
    "Now we want to deploy the image onto GCP using the Kubernetes service. The first step is to save your image to a Docker registry that can interface with the orchestration system. The GCP version of this registry service is called Container Registry. \n",
    "\n",
    "Make sure we turn on the `Google Container Registry API` and `Kubernetes Engine API` by enabling them in the GCP console.\n",
    "\n",
    "#### Push the Image\n",
    "\n",
    "We need the GCP credentials JSON file that we set up in the `Step 1` Chapter. Then we log in GCP Container Registry at https://us.gcr.io .\n",
    "\n",
    "```\n",
    "$ cat newacc_gcp_credential.json | sudo docker login -u _json_key --password-stdin https://us.gcr.io\n",
    "$ sudo docker tag echo_service us.gcr.io/scalable-model-piplines/echo_service \n",
    "$ sudo docker push us.gcr.io/scalable-model-piplines/echo_service\n",
    "```\n",
    "\n",
    "Log in on Windows OS:\n",
    "\n",
    "```\n",
    "$ type newacc_gcp_credential.json | docker login -u _json_key --password-stdin https://us.gcr.io\n",
    "```\n",
    "\n",
    "#### Run Service on Kubernetes\n",
    "\n",
    "On the `Google Kubernetes Engine` main page:\n",
    "\n",
    "1. Click \"Deploy Container\".\n",
    "2. Select \"Existing Container Image\".\n",
    "3. Choose \"echo_service:latest\".\n",
    "4. Assign an application name \"echo-gke\".\n",
    "5. Click \"Deploy\".\n",
    "\n",
    "When GKE ran the Docker image, it started a cluster and we can set up `Services` which are sets of Pods with a network endpoint that can be used for discovery and load balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea741e1e",
   "metadata": {},
   "source": [
    "![Kubernetes_Cluster_Nodes.jpg](images/Kubernetes_Cluster_Nodes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e63f5",
   "metadata": {},
   "source": [
    "### 1.7 Discover the Cluster\n",
    "\n",
    "In `Workloads` menu> select *echo-gke* > menu ACTION > run *EXPOSE* to expose the cluster to external traffic.\n",
    "\n",
    "Now we choose `Load balancer` for Service type which maps to port 80 to our gunicorn server.\n",
    "![echo-gke-service-balancer.png](images/echo-gke-service-balancer.png)\n",
    "\n",
    "By calling `http://34.70.212.160/predict?range=1` it will return JSON result of predictions.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "result = requests.get(\"http://34.70.212.160/predict?range=1\")\n",
    "print(result.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb6dfa",
   "metadata": {},
   "source": [
    "### 1.8 Automatically Scaling with `autoscale`\n",
    "\n",
    "For the Kubernetes cluster, we can use `kubectl` to make our cluster automatically scale up when a condition is met. For the example below, when all CPU utilization is up to 80%, the cluster is adding more nodes until it reaches --max=5 nodes.\n",
    "\n",
    "```\n",
    "kubectl autoscale deployment echo-gke --cpu-percent=80 --min=2 --max=5\n",
    "```\n",
    "\n",
    "In GKE, there is an option to turn on the `autoscale`\n",
    "\n",
    "1. Go to GKE UI, select **Workloads** on the left menu\n",
    "\n",
    "2. Select the deployment of the workloads, in this case, it is **prediction**\n",
    "\n",
    "3. On top menu, click *ACTIONS* and choose **Autoscale**\n",
    "\n",
    "![gke-autoscale](images/gke-autoscale.png)\n",
    "\n",
    "4. There are two choices: Horizontal vs Vertical pod autoscaling. If choosing Horizontal Scaling:\n",
    "\n",
    "![gke-autoscale 2](images/gke-autoscale_2.png)\n",
    "\n",
    "\n",
    "5. Select what you want and click **SAVE** to update the cluster immediately.\n",
    "\n",
    "**Traffic-based autoscaling**: Besides *pod autoscaling*, we also can scale up the system based on traffic utilization signals. Traffic-based autoscaling is enabled by the Gateway controller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5b893",
   "metadata": {},
   "source": [
    "### 1.9 Note\n",
    "\n",
    "In this example, I put trained models inside the container. In the next chapter, we can put these trained models on external storage (i.e. `Google Cloud Storage`). Whenever the predictor is called, it loads the trained model and runs the prediction. \n",
    "\n",
    "Hence, we only need to set up a Container once and focus on developing model pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ea15c",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6665bf",
   "metadata": {},
   "source": [
    "## 2. Batch Model Pipelines\n",
    "\n",
    "Batch model pipelines automatically perform a sequence of tasks in order to train and store results for a Machine Learning model. In a batch process, we perform a set of operations that store trained models and predictions that are later served by different applications.\n",
    "\n",
    "However, when working with the pipelines, any issues should be resolved quickly. Because there are many tasks and components involved in a pipeline, issues can occur at any part of the system. Hence, we need a system in place that can send alerts to the team that owns the pipeline and rerun portions of the model pipeline to resolve any issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898d862",
   "metadata": {},
   "source": [
    "### 2.1 Workflow tools\n",
    "\n",
    "Workflow tools provide a solution for managing these types of problems in model pipelines and are responsible for:\n",
    "\n",
    "* Running tasks\n",
    "* Provisioning resources\n",
    "* Monitoring the status of tasks\n",
    "\n",
    "There are several open-source tools for building workflows including Airflow, Luigi, MLflow, and Pentaho Kettle. We focus on Apache Airflow, which `Google Cloud Composer` is built on it, in this project.\n",
    "\n",
    "### 2.2 Model Pipelines: \n",
    "\n",
    "There are typically two types of batch model pipelines:\n",
    "\n",
    "* Persistent: The models are trained and saved to a (binary) file. Any applications/pipelines can reload this model to run prediction.\n",
    "* Transient: When an app/pipeline wants to do prediction, it builds and trains the model, then predicts.\n",
    "\n",
    "### 2.3 Benefits over Crons\n",
    "\n",
    "There are a few situations where workflow tools provide benefits over using cron directly:\n",
    "\n",
    "* Dependencies: Workflow tools define graphs of operations, making [dependencies explicit](https://cloud.google.com/deployment-manager/docs/configuration/create-explicit-dependencies).\n",
    "* Backfills: It may be necessary to run an ETL for a range of different dates on old data (late arrival of data). \n",
    "* Versioning: Most workflow tools integrate with version control systems to manage graphs.\n",
    "* Alerting: These tools can send out emails or generate PageDuty alerts when failures occur.\n",
    "\n",
    "Workflow tools: One of the key benefits is the ability to handle DAG configuration as code, which enables code reviews and version control for workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974c177",
   "metadata": {},
   "source": [
    "### 2.4 Cloud Dataflow\n",
    "\n",
    "`Google Cloud Dataflow` provides a useful framework for scaling up sklearn models to massive datasets. Instead of fitting all the input data into a data frame, we can score each record individually in the process function and use Apache Beam to stream these outputs to a data sink, such as BigQuery.\n",
    "\n",
    "There are a lot of [examples/tutorials for TensorFlow](https://cloud.google.com/dataflow/docs/tutorials/python-ml-examples) work in Cloud Dataflow for batch predictions [here](https://cloud.google.com/dataflow/docs/tutorials/python-ml-examples).\n",
    "\n",
    "The core component in Dataflow is a pipeline that defines the operations to perform as part of a workflow:\n",
    "\n",
    "* Pipeline: defines the set of operations to perform as part of a Dataflow job.\n",
    "* Collection: the interface between different stages in a workflow. The input to any step in a workflow is a collection of objects and the output is a new collection of objects.\n",
    "* DoFn: an operation to perform on `each element` in a collection, resulting in a new collection.\n",
    "* Transform: an operation to perform on `sets of elements` in a collection, such as aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7a83a",
   "metadata": {},
   "source": [
    "### 2.5 Apache Beam\n",
    "\n",
    "Cloud Dataflow builds upon the open-source Apache Beam library, making it usable in other cloud environments. One of the main reasons for using Beam is the ability to switch between multiple runners such as Apache Spark, Apache Flink, Samza, and Google Cloud Dataflow because Apache Beam looks like a framework.\n",
    "\n",
    "![Apache-Beam.png](images/Apache-Beam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc1749",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f1caa",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d58f4",
   "metadata": {},
   "source": [
    "## 3. Streaming Model Pipelines\n",
    "\n",
    "Many organizations are now using streaming platforms in order to build real-time data pipelines that transform streams of data and move data between different components in a cloud environment. These platforms are typically distributed and provide fault tolerance for streaming data. \n",
    "\n",
    "To connect different systems, we can use Kafka for a self-host solution or cloud services such as Google Pub/Sub or Amazon Kinesis.\n",
    "\n",
    "### 3.1 Types of Prediction\n",
    "\n",
    "* Passive prediction (like log or update): when the user made an action (watch, view, play, buy, sell an item), we can update the recommendation list for that user for the next serving. The predictions are saved to a database or data store.\n",
    "\n",
    "* Active prediction: A client asks for a prediction and sends a request to modeling, the result might not need right away (pre-generating prediction in the background). But the results are sent back to the client and are not probably saved to a database.\n",
    "\n",
    "### 3.2 Benefits\n",
    "\n",
    "Although the *Active prediction* sounds like Lambda or Cloud Function, there are keys different when using Cloud Dataflow:\n",
    "\n",
    "* It is easy to connect with other components in cloud platforms. We can push data to BigQuery for storage or forward output to another message consumer.\n",
    "\n",
    "* Able to use distributed tools, such as PySpark, in contrast to the endpoint-based approaches that service requests in isolation.\n",
    "\n",
    "* More tools and different programming languages supported. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be478ec",
   "metadata": {},
   "source": [
    " ### 3.3 Google Pub/Sub\n",
    " \n",
    "Unlike Kafka, PubSub uses separate concepts for producer and consumer data sources. In Kafka, you can publish and subscribe to a topic directly, while in PubSub consumers subscribe to subscriptions rather than directly subscribing to topics. With PubSub, you first set up a topic and then create one or more subscriptions that listen to this topic.\n",
    "\n",
    "In PubSub UI in the GCP Console, let's create a topic ID *sale-range*\n",
    "\n",
    "In this topic, we create a Subscription ID *predict-range*. There are three Delivery types:\n",
    "\n",
    "* Pull: your subscriber client initiates requests to a Pub/Sub server to retrieve messages.\n",
    "\n",
    "* Push: a Pub/Sub server initiates a request to your subscriber client to deliver messages.\n",
    "\n",
    "* Write to BigQuery: A BigQuery subscription writes messages to an existing BigQuery table as they are received. You need not configure a separate subscriber client.\n",
    "\n",
    "For pull, the subscribers must request delivery. For other subscription types, Pub/Sub delivers messages as soon as they are published."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22148203",
   "metadata": {},
   "source": [
    "### 3.4 Demo of Connection\n",
    " \n",
    "#### how to read messages from the subscription\n",
    " \n",
    " ```python\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "subscription_path = subscriber.subscription_path(\"google_project_name\", \"Subscription_ID\")\n",
    "\n",
    "def callback(message):\n",
    "    print(message.data)\n",
    "    message.ack()\n",
    "\n",
    "subscriber.subscribe(subscription_path, callback=callback)\n",
    "```\n",
    "\n",
    "#### publish a message to a topic\n",
    "\n",
    "```python\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(\"google_project_name\", \"Topic_ID\")\n",
    "\n",
    "data = \"Hello World!\".encode('utf-8')\n",
    "publisher.publish(topic_path, data=data)\n",
    "```\n",
    "\n",
    "We need to encode the message in utf-8 format before publishing the message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61d22a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129f5be",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
