{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4c49ac",
   "metadata": {},
   "source": [
    "# Batch Model Pipelines: Training and Predicting\n",
    "\n",
    "## 0. Preparing Data in BigQuery\n",
    "\n",
    "I use *db/store-sales-time-series-forecasting_training.csv* which sales were avalable from 2016-08-01 until 2017-07-31 and insert to BigQuery in Console by uploading CSV file.\n",
    "\n",
    "* **Dataset ID** = store_sales\n",
    "\n",
    "* **Table ID** = simplified_data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd80f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'newacc_gcp_credential.json'\n",
    "query = \"\"\"\n",
    "    SELECT date, store_nbr, family, sales, onpromotion\n",
    "    FROM `scalable-model-piplines.store_sales.simplified_data_table` \n",
    "    limit 100    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab22c6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>1</td>\n",
       "      <td>PERSONAL CARE</td>\n",
       "      <td>200.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>1</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>1909.962</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-07</td>\n",
       "      <td>1</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>1016.462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-09</td>\n",
       "      <td>1</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>2044.128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>1</td>\n",
       "      <td>PERSONAL CARE</td>\n",
       "      <td>47.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store_nbr         family     sales  onpromotion\n",
       "0  2016-08-03          1  PERSONAL CARE   200.000            0\n",
       "1  2016-08-04          1        PRODUCE  1909.962            0\n",
       "2  2016-08-07          1        PRODUCE  1016.462            0\n",
       "3  2016-08-09          1        PRODUCE  2044.128            0\n",
       "4  2016-08-14          1  PERSONAL CARE    47.000            0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "train_data = client.query(query).to_dataframe()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5117c0",
   "metadata": {},
   "source": [
    "## 1. Training Model from BigQuery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c9e5b",
   "metadata": {},
   "source": [
    "### 1.1 Model Trainer\n",
    "\n",
    "A full code for demonstration of `Model Trainer` is in */demo-code/prediction_service/model_trainer.py*\n",
    "\n",
    "1. After getting DB from BigQuery, we do *Feature Engineering* with supporting Seasonality.\n",
    "\n",
    "2. Fit the model and temporarily save the trained model and its information to *tmp/* directory. \n",
    "\n",
    "3. Upload the trained model to Google Cloud Storage using *gcsfs* library\n",
    "\n",
    "```python\n",
    "def upload_File_to_Cloud_Storage(src_dir: str, gcs_dst: str, recursive=False):\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    fs.put(src_dir, gcs_dst, recursive=recursive)\n",
    "\n",
    "bucket_name = \"gcs://scalable-model-piplines-trained_model/\"\n",
    "\n",
    "upload_File_to_Cloud_Storage(path + model_name + today, \n",
    "                             bucket_name + model_name + today, recursive=True)\n",
    "\n",
    "upload_File_to_Cloud_Storage(path + dp_name + today + '.pkl', \n",
    "                             bucket_name + dp_name + today + '.pkl')\n",
    "```\n",
    "\n",
    "4. Clean up the temporary files. \n",
    "\n",
    "I assume the model is trained every day, so the file name of the model would be in a format of \"sale_forecasting_sklearn\" + \"today\", for example, *sale_forecasting_sklearn2023-02-28*\n",
    "\n",
    "We can specify the model version or overwrite the previous model to make it persistent and consistent.\n",
    "\n",
    "We also ensure the model was trained properly and accurately before moving to production. We also run some services to double-check the model and send alerts to developers about any rising issues. Cross-check is a good practice to ensure everything is going well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50b641",
   "metadata": {},
   "source": [
    "### 1.2 Containerize the Model Trainer\n",
    "\n",
    "```bash\n",
    "$ docker image build -t \"prediction_service\" .\n",
    "\n",
    "$ docker run -d -p 8080:80 prediction_service\n",
    "\n",
    "$ docker tag prediction_service us.gcr.io/scalable-model-piplines/prediction_service\n",
    "\n",
    "$ docker push us.gcr.io/scalable-model-piplines/prediction_service\n",
    "```\n",
    "\n",
    "Now we can deploy this image in `Google Kubernetes Engine`, expose it with `load balancer`, and map the port to 80.\n",
    "\n",
    "![gke-prediction-service-deployment](images/gke-prediction-service-deployment.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d86d4cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': True}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "result = requests.get(\"http://34.69.150.86/run\")\n",
    "print(result.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee769b4",
   "metadata": {},
   "source": [
    "Now the Model Trainer can be called via HTTP Web Endpoint in the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43a9c0",
   "metadata": {},
   "source": [
    "### 1.3 Scheduler with `Cloud Composer`\n",
    "\n",
    "After running the GKE of Model Trainer, we set up a schedule to run this service every day at 00:00 AM. It is possible to self-host Airflow on Kubernetes, but it can be complex to set up. There are also fully-managed versions of Airflow available for cloud platforms such as Cloud Composer on GCP.\n",
    "\n",
    "1. Enable `Google Cloud Composer API` and create a new Environment named *run-model-trainer-everday*.\n",
    "\n",
    "2. Add DAG to GKE cluster which runs Model Trainer. In `Cloud Composer` UI >> select Composer cluster >> \n",
    "\n",
    "For the purpose of demonstration, we call `Model Trainer` via HTTP Web Endpoint. Below is the snippet code of `task1_run_Model_Trainer.py`\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    import requests\n",
    "    result = requests.get(\"http://34.69.150.86/run\")\n",
    "```\n",
    "\n",
    "However, `Cloud Composer` can do so much more. For example, it can run a  Python app to build and train a model, then run another (Python) app to either deploy the trained model to a storage or containerize it to a Docker image.\n",
    "\n",
    "GCP provides `Cloud Build` to run a job to build and push the Docker image to Artifact Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51bc33",
   "metadata": {},
   "source": [
    "#### Multiple Tasks with Multiple Python Scripts\n",
    "\n",
    "`Cloud Composer` runs and manages scripts in a bucket of `Google Storage`. In this case, it's us-west1-run-model-trainer--5f3a0641-bucket\n",
    "\n",
    "```\n",
    "us-west1-run-model-trainer--5f3a0641-bucket/\n",
    "    dags/\n",
    "        train_model.py\n",
    "        tasks/\n",
    "             task1_run_Model_Trainer.py\n",
    "             task2_upload_Model.py\n",
    "```\n",
    "\n",
    "The template for `train_model.py` is \n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from tasks import task1_run_Model_Trainer, task2_upload_Model\n",
    "\n",
    "default_dag_args = {}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"train_model\",\n",
    "    default_args=default_dag_args,\n",
    "    start_date=datetime(2023, 2, 25, 0, 0),\n",
    "    #schedule_interval=timedelta(days=1), # every day\n",
    "    # At 08:00 AM  every day\n",
    "    schedule_interval='0 8 * * *'\n",
    ") as dag:\n",
    "    do_stuff1 = PythonOperator(\n",
    "        task_id=\"task_1\",\n",
    "        python_callable=task1_run_Model_Trainer.main,  # assume entrypoint is main()\n",
    "    )\n",
    "    do_stuff2 = PythonOperator(\n",
    "        task_id=\"task_2\",\n",
    "        python_callable=task2_upload_Model.main,  # assume entrypoint is main()\n",
    "    )\n",
    "    do_stuff1 >> do_stuff2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa715d",
   "metadata": {},
   "source": [
    "To make two actions sequential, we use \">>\", for example *do_stuff1 >> do_stuff2*\n",
    "\n",
    "Right after putting `train_model.py` into `dags` folder in Google Storage bucket of Cloud Composer, `Cloud Composer` updates the new DAG and runs it if needed.\n",
    "\n",
    "![Google_Composer.png](images/Google_Composer_dags.png)\n",
    "\n",
    "`Cloud Composer` also supports UI for Airflow:\n",
    "\n",
    "![Google_Composer_aiflow_ui.png](images/Google_Composer_aiflow_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df44bd",
   "metadata": {},
   "source": [
    "### 1.4 Batch Training Model\n",
    "\n",
    "In case of very large records in BigQuery, we can do batch training by feed the model batch by batch. BigQuery supports *Batch query jobs*. The demo code:\n",
    "\n",
    "```python\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    priority=bigquery.QueryPriority.BATCH\n",
    ")\n",
    "query_job = client.query(sql, job_config=job_config)  # Make an API request.\n",
    "while(query_job.state != 'DONE'):\n",
    "    # DO Somthing\n",
    "    query_job = client.get_job(\n",
    "        query_job.job_id, location=query_job.location\n",
    "    )  # Make a next API request.\n",
    "    \n",
    "```\n",
    "\n",
    "There are several Python libraries which support batching. \n",
    "\n",
    "\n",
    "### 1.5 Tensorflow Model\n",
    "\n",
    "If the model built with Tensorflow and needs query data from BigQuery, we can use [`tfio.bigquery.BigQueryClient`](https://www.tensorflow.org/io/api_docs/python/tfio/bigquery/BigQueryClient).\n",
    "\n",
    "[There is an example of using it here.](https://www.tensorflow.org/io/tutorials/bigquery)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44079ae5",
   "metadata": {},
   "source": [
    "### 1.6 AI Platform Training\n",
    "\n",
    "Besides general cloud environments, GCP provides `AI Platform Training` to run TensorFlow, scikit-learn, and XGBoost training applications in the cloud. AI Platform Training provides the dependencies required to train machine learning models using these hosted frameworks in their runtime versions. Additionally, we can use custom containers to run training jobs with other machine learning frameworks.\n",
    "\n",
    "`AI Platform Training` strongly supports [distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a679f8",
   "metadata": {},
   "source": [
    "## 2. Spark: Batch Model with PySpark and MLlib\n",
    "\n",
    "[`Dataproc`](https://cloud.google.com/dataproc) is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. Use Dataproc for data lake modernization, ETL, and secure data science, at scale, integrated with Google Cloud, at a fraction of the cost.\n",
    "\n",
    "### 2.1 MLlib\n",
    "\n",
    "MLlib contains many algorithms including:\n",
    "\n",
    "\n",
    "* Classification: logistic regression, naive Bayes,...\n",
    "* Regression: generalized linear regression, survival regression,...\n",
    "* Decision trees, random forests, and gradient-boosted trees\n",
    "* Recommendation: alternating least squares (ALS)\n",
    "* Clustering: K-means, Gaussian mixtures (GMMs),...\n",
    "* Topic modeling: latent Dirichlet allocation (LDA)\n",
    "* Frequent itemsets, association rules, and sequential pattern mining\n",
    "\n",
    "### 2.1 Train Model with MLlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be62d2",
   "metadata": {},
   "source": [
    "This code below is `pyspark-training-model/run_trainning.py`\n",
    "\n",
    "```python\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "# Define a function that collects the features of interest\n",
    "# (date, store_nbr) into a vector.\n",
    "# Package the vector in a tuple containing the label (`sales`) for that row.\n",
    "def vector_from_inputs(r):\n",
    "      return (float(r[\"sales\"]), Vectors.dense(time.mktime(r[\"date\"].timetuple()),\n",
    "                                            float(r[\"store_nbr\"]),))\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Read the data from BigQuery as a Spark Dataframe.\n",
    "sales_data = spark.read.format(\"bigquery\").option('project','scalable-model-piplines').option(\n",
    "    \"table\", \"store_sales.simplified_data_table\").load()\n",
    "# Create a view so that Spark SQL queries can be run against the data.\n",
    "sales_data.createOrReplaceTempView(\"sales_data\")\n",
    "\n",
    "clean_data = spark.sql(query)\n",
    "# Create an input DataFrame for Spark ML using the above function.\n",
    "training_data = clean_data.rdd.map(vector_from_inputs).toDF([\"label\",\n",
    "                                                             \"features\"])\n",
    "training_data.cache()\n",
    "# Construct a new LinearRegression object and fit the training data.\n",
    "lr = LinearRegression(maxIter=5, regParam=0.2, solver=\"normal\")\n",
    "model = lr.fit(training_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c3a5c",
   "metadata": {},
   "source": [
    "### 2.2 Submit Job in `Dataproc`\n",
    "\n",
    "To run this Python code, we need to submit it as job to the Dataproc service.\n",
    "\n",
    "1. Copy the code file (run_trainning.py) to a bucket amd get its full path file (gs://scalable-model-piplines-dataproc-model-trainer/run_trainning.py)\n",
    "\n",
    "2. In the main page of Dataproc UI, choose `SUBMIT JOB` \n",
    "\n",
    "3. Select PySpark as the Job type and insert the link of the Python file.\n",
    "\n",
    "4. To use BigQuery service, we need to add *spark-bigquery-connector*: Insert `gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar` in the **Jar files** field.\n",
    "\n",
    "If nothing wrong, we will get a status of success.\n",
    "\n",
    "![spark-job-submit-success.png](images/spark-job-submit-success.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352590bc",
   "metadata": {},
   "source": [
    "### 2.3 Scheduler\n",
    "\n",
    "It is possible to use `Cloud Composer` to schedule jobs in Dataproc by running [`Dataproc Serverless`](https://cloud.google.com/dataproc-serverless/docs/overview) workloads. A guideline was [provided here](https://cloud.google.com/composer/docs/composer-2/run-dataproc-workloads).\n",
    "\n",
    "It's also possible to [dynamically create DataProc cluster using Cloud Composer](https://www.linkedin.com/pulse/dynamically-create-dataproc-cluster-using-cloud-composer-kanungo) to schedule a job and once the job is finished how we can decommission the cluster automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a52d31",
   "metadata": {},
   "source": [
    "## 3. Batch Predicting Model\n",
    "\n",
    "### 3.1 Google Cloud Dataflow\n",
    "\n",
    "Dataflow is a tool for building data pipelines that can run locally or scale up to large clusters in a managed environment. We briefly talked about it in the last Chapter.\n",
    "\n",
    "Now we will use Apache Beam to build a Predicting Service in the `Google Cloud Dataflow` environment\n",
    "\n",
    "### 3.2 Process Data with `DoFn`\n",
    "\n",
    "Below is a template of how we define a class of `DoFn` to predict each row (element) from data (for example, loaded from BigQuery). Because it separates the whole data, it can scale up to a massive processing data pipeline.\n",
    "\n",
    "#### Prediction\n",
    "\n",
    "```python\n",
    "import apache_beam as beam\n",
    "\n",
    "class ApplyDoFn(beam.DoFn):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "     \n",
    "    def process(self, element):\n",
    "        if self._model is None:\n",
    "            self._model = LOAD_MODEL\n",
    "        \n",
    "        new_x = TRANSFORM(element)\n",
    "        prediction = self._model.predict(new_x)[0]\n",
    "        return [ { 'guid': element['guid'], 'prediction': prediction } ]\n",
    "    \n",
    "predictions = data | 'Apply Model' >> beam.ParDo(ApplyDoFn())\n",
    "```\n",
    "\n",
    "Now we can save the prediction results anywhere, it could be another table of BigQuery or a NoSQL DB such as `Google Cloud Datastore`\n",
    "\n",
    "#### Save to `Cloud Datastore`\n",
    "\n",
    "```python\n",
    "class PublishDoFn(beam.DoFn):\n",
    "    \n",
    "    def __init__(self):\n",
    "        from google.cloud import datastore       \n",
    "        self._ds = datastore\n",
    "    \n",
    "    def process(self, element):\n",
    "        client = self._ds.Client()\n",
    "        entity = self._ds.Entity(client.key())\n",
    "        entity['prediction'] = element['prediction']         \n",
    "        entity['time'] = element['time']\n",
    "        client.put(entity)\n",
    "\n",
    "predictions | 'Create entities' >> beam.ParDo(PublishDoFn())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b079f",
   "metadata": {},
   "source": [
    "### 3.3 Predicting Service \n",
    "\n",
    "To create a workflow with Beam, we use the pipe syntax in Python to chain different steps together. The result is a DAG of operations to perform that can be distributed across machines in a cluster.\n",
    "\n",
    "```python\n",
    "# define the pipeline steps\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "data = p | 'Read Data' >> beam.io.Read(DATA_SOURCE)\n",
    "scored = data | 'Apply Model for Each Element/User' >> beam.ParDo(ApplyDoFn())\n",
    "scored | 'Save to BigQuery' >> beam.io.Write(beam.io.WriteToBigQuery(\n",
    "                'prediction_Table', 'dataset_ID', schema = schema,\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))\n",
    "scored | 'Save to Other DB like Datastore' >> beam.ParDo(PublishDoFn())\n",
    "\n",
    "# run the pipeline\n",
    "result = p.run()\n",
    "result.wait_until_finish()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf65f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
